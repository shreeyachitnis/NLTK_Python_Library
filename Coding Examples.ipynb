{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4723c4",
   "metadata": {},
   "source": [
    "Name: Shreeya Chitnis \n",
    "Library: NLTK\n",
    "URL: https://github.com/shreeyachitnis/NLTK\n",
    "Description: NLTK, short for Natural Language Toolkit, is a comprehensive Python library designed to facilitate the exploration, processing, and analysis of human language data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfdeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of customer reviews\n",
    "df = pd.read_csv('data/Womens Clothing E-Commerce Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the 'Review Text' column\n",
    "df.dropna(subset=['Review Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the length of the dataset as it takes a lot of time to process \n",
    "df = df.iloc[:len(df)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e436cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the 'review' column\n",
    "df['tokens'] = df['Review Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "# Perform sentiment analysis using NLTK's VADER\n",
    "df['sentiment_score'] = df['tokens'].apply(lambda tokens: sid.polarity_scores(' '.join(tokens))['compound'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution for Sentiment Scores  \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['sentiment_score'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd28422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key insights from the text data\n",
    "def extract_insights(tokens):\n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(tokens)\n",
    "    \n",
    "    # Get the most common words\n",
    "    most_common_words = fdist.most_common(10)\n",
    "    \n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply insights extraction to the 'tokens' column\n",
    "df['insights'] = df['tokens'].apply(extract_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530817eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insights_list = []\n",
    "# Iterate through the DataFrame and collect insights\n",
    "for idx, row in df.iterrows():\n",
    "    review_number = idx + 1\n",
    "    insights = row['insights']\n",
    "    insights_list.append({'Review': review_number, 'Insights': insights})\n",
    "\n",
    "# Create a new DataFrame from the list\n",
    "insights_df = pd.DataFrame(insights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f129eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of movie reviews\n",
    "df = pd.read_csv('data/movie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1262ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the length of the dataset as it takes a lot of time to process \n",
    "df = df.iloc[:len(df)//8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fadf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, Lowercasing, Removing Stopwords and Punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['preprocessed_text'] = df['text'].apply(lambda text: ' '.join([word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ceacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction: TF-IDF Vectorization\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier (e.g., Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
